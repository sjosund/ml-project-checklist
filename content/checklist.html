<section data-type="chapter">

    <h1>Machine Learning Project Checklist</h1>

        <p id="introduction">
            The items on this checklist come from various sources, such as <a href="https://www.deeplearning.ai/machine-learning-yearning/"> Machine Learning Yearning</a>, <a href="https://fullstackdeeplearning.com/">Full Stack Deep Learning</a>, <a href="https://mlpowered.com/book/"> Building Machine Learning Powered Applications</a>, and also from my own personal experience. This is work in progress, and contributions are welcome. If you have any additions, please submit a PR to <a href="https://github.com/sjosund/ml-project-checklist">this repo</a>.
        </p>
    <h2>Before modelling</h2>
    <h3>Project</h3>
        <input type="checkbox"> The project has a clear, codified business goal/metric.</input><br>
        <input type="checkbox"> There is a person who is ultimately responsible for the success/failure of the project.</input><br>
        <input type="checkbox"> We have a plan for how to reach a first deployed end product as fast as possible.</input><br>
        <input type="checkbox"> We have decided on how and when to keep the team in sync (daily/weekly standups, retrospectives, planning meetings, etc)</input><br>

    
    <h3>Problem understanding</h3>
        <input type="checkbox"> We have decided on one single metric on which to rank my models.</input><br>
        <input type="checkbox"> We have clarified the costs of the different kinds of erroneous predictions.</input><br>
        <input type="checkbox"> We have an understanding of how good performance is "good enough"</input><br>
        <input type="checkbox"> We know the constraints in serving time w.r.t. memory usage.</input><br>
        <input type="checkbox"> We know the constraints in serving time w.r.t. latency.</input><br>
        <input type="checkbox"> We know the constraints in serving time w.r.t. throughput.</input><br>
        <input type="checkbox"> We know if we’re doing streaming- or batch prediction.</input><br>
        <input type="checkbox"> We understand the current state of ML applied to the problem we’re trying to solve.</input><br>
        <input type="checkbox"> We have an idea of how important freshness is. How often will we need to change the model?</input><br>
        <input type="checkbox"> We have domain experts who can help us understand the problem and error modes.</input><br>
        <input type="checkbox"> We know where the model will be deployed (server / client, browser / on device)</input><br>
    <h3>Data</h3>
        <input type="checkbox"> I have selected a dev- and test set that are reflective of the real task I’m trying to solve.</input><br>
        <input type="checkbox"> My dev- and test sets are from the same distribution.</input><br>
        <input type="checkbox"> My dev set is large enough, so that I can detect improvements to the desired accuracy.</input><br>
        <input type="checkbox"> We understand how to split the data into train/val/test to avoid data leakage.</input><br>
        <input type="checkbox"> If we need to collect data, we know how difficult and costly it will be to collect and annotate.</input><br>
        <input type="checkbox"> We have a plan for how to store and version our data, dataset splits, models, and change in annotations.</input><br>
        <input type="checkbox"> I get a reasonable <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf">“ML Test Score”</a>, table 1.</input><br>

    <h2>Modelling</h2>
        <input type="checkbox"> I have one or several well thought out baselines in place. These are not good enough, so there’s an actual need to use ML.</input><br>
        <input type="checkbox"> There’s a metrics webpage where I can compare runs and the url is ___________________.</input><br>
        <input type="checkbox"> We can (approximately) reproduce a model if needed.</input><br>
        <input type="checkbox"> I get a reasonable <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf">“ML Test Score”</a>, table 2.</input><br>

    <h2>Deployment</h2>
        <input type="checkbox"> We have CI in place.</input><br>
        <input type="checkbox"> We have tests for the full training pipeline.</input><br>
        <input type="checkbox"> We have validation tests.</input><br>
        <input type="checkbox"> We have functionality tests.</input><br>
        <input type="checkbox"> We have unit tests.</input><br>
        <input type="checkbox"> We have CD in place.</input><br>
        <input type="checkbox"> We have CT in place.</input><br>
        <input type="checkbox"> Blue/green deployment in place.</input><br>
        <input type="checkbox"> We can deploy a model in shadow mode.</input><br>
        <input type="checkbox"> Monitoring in place for memory consumption.</input><br>
        <input type="checkbox"> Monitoring in place for CPU consumption.</input><br>
        <input type="checkbox"> Monitoring in place for latency.</input><br>
        <input type="checkbox"> Monitoring in place for downtime.</input><br>
        <input type="checkbox"> Monitoring in place for requests per second.</input><br>
        <input type="checkbox"> Monitoring in place for prediction confidence over time.</input><br>
        <input type="checkbox"> We have a way of detecting if a model will fail on a given datapoint, and a corresponding fallback.</input><br>
        <input type="checkbox"> I get a reasonable <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf">“ML Test Score”</a>, table 3-4.</input><br>

</div>
