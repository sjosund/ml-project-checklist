<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>ML Project Checklist</title>
    
    <link rel="stylesheet" href="assets/html.css">

  </head>
  <body data-type="book">
    <div class="container">
      <section data-type="chapter" id="machine-learning-project-checklist-4bp5hGV">

    <h1>Machine Learning Project Checklist</h1>

        <p id="introduction">
            The items on this checklist come from various sources, such as <a href="https://www.deeplearning.ai/machine-learning-yearning/"> Machine Learning Yearning</a>, <a href="https://fullstackdeeplearning.com/">Full Stack Deep Learning</a>, <a href="https://mlpowered.com/book/"> Building Machine Learning Powered Applications</a>, and also from my own personal experience. This is work in progress, and contributions are welcome. If you have any additions, please submit a PR to <a href="https://github.com/sjosund/ml-project-checklist">this repo</a>.
        </p>
    <h2>Before modelling</h2>
    <h3>Project</h3>
        <input type="checkbox"> The project has a clear, codified business goal/metric.<br>
        <input type="checkbox"> There is a person who is ultimately responsible for the success/failure of the project.<br>
        <input type="checkbox"> We have a plan for how to reach a first deployed end product as fast as possible.<br>
        <input type="checkbox"> We have decided on how and when to keep the team in sync (daily/weekly standups, retrospectives, planning meetings, etc)<br>

    
    <h3>Problem understanding</h3>
        <input type="checkbox"> We have decided on one single metric on which to rank my models.<br>
        <input type="checkbox"> We have clarified the costs of the different kinds of erroneous predictions.<br>
        <input type="checkbox"> We have an understanding of how good performance is &quot;good enough&quot;<br>
        <input type="checkbox"> We know the constraints in serving time w.r.t. memory usage.<br>
        <input type="checkbox"> We know the constraints in serving time w.r.t. latency.<br>
        <input type="checkbox"> We know the constraints in serving time w.r.t. throughput.<br>
        <input type="checkbox"> We know if we&#x2019;re doing streaming- or batch prediction.<br>
        <input type="checkbox"> We understand the current state of ML applied to the problem we&#x2019;re trying to solve.<br>
        <input type="checkbox"> We have an idea of how important freshness is. How often will we need to change the model?<br>
        <input type="checkbox"> We have domain experts who can help us understand the problem and error modes.<br>
        <input type="checkbox"> We know where the model will be deployed (server / client, browser / on device)<br>
    <h3>Data</h3>
        <input type="checkbox"> I have selected a dev- and test set that are reflective of the real task I&#x2019;m trying to solve.<br>
        <input type="checkbox"> My dev- and test sets are from the same distribution.<br>
        <input type="checkbox"> My dev set is large enough, so that I can detect improvements to the desired accuracy.<br>
        <input type="checkbox"> We understand how to split the data into train/val/test to avoid data leakage.<br>
        <input type="checkbox"> If we need to collect data, we know how difficult and costly it will be to collect and annotate.<br>
        <input type="checkbox"> We have a plan for how to store and version our data, dataset splits, models, and change in annotations.<br>
        <input type="checkbox"> I get a reasonable <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf">&#x201C;ML Test Score&#x201D;</a>, table 1.<br>

    <h2>Modelling</h2>
        <input type="checkbox"> I have one or several well thought out baselines in place. These are not good enough, so there&#x2019;s an actual need to use ML.<br>
        <input type="checkbox"> There&#x2019;s a metrics webpage where I can compare runs and the url is ___________________.<br>
        <input type="checkbox"> We can (approximately) reproduce a model if needed.<br>
        <input type="checkbox"> I get a reasonable <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf">&#x201C;ML Test Score&#x201D;</a>, table 2.<br>

    <h2>Deployment</h2>
        <input type="checkbox"> We have CI in place.<br>
        <input type="checkbox"> We have tests for the full training pipeline.<br>
        <input type="checkbox"> We have validation tests.<br>
        <input type="checkbox"> We have functionality tests.<br>
        <input type="checkbox"> We have unit tests.<br>
        <input type="checkbox"> We have CD in place.<br>
        <input type="checkbox"> We have CT in place.<br>
        <input type="checkbox"> Blue/green deployment in place.<br>
        <input type="checkbox"> We can deploy a model in shadow mode.<br>
        <input type="checkbox"> Monitoring in place for latency.<br>
        <input type="checkbox"> Monitoring in place for downtime.<br>
        <input type="checkbox"> Monitoring in place for requests per second.<br>
        <input type="checkbox"> Monitoring in place for prediction confidence over time.<br>
        <input type="checkbox"> We have a way of detecting if a model will fail on a given datapoint, and a corresponding fallback.<br>
        <input type="checkbox"> I get a reasonable <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/aad9f93b86b7addfea4c419b9100c6cdd26cacea.pdf">&#x201C;ML Test Score&#x201D;</a>, table 3-4.<br>


</section>

    </div>
  </body>
</html>
